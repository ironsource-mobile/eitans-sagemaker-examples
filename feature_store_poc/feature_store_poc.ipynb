{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker FeatureStore - IS PoC\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook.\n",
    "\n",
    "In this notebook, we will load device embedding data into AWS Feature Store.\n",
    "A preliminary <A HREF=\"https://github.com/SupersonicAds/sonic-ftrl-api/blob/27d8815cf22bcc28c1b4de1bde5ef87bcae459a1/sequencing/sequence_modeling.ipynb\">ETL</A> will produce sequences of devices.\n",
    "These sequences will later be converted to vector embedding/representations by https://github.com/SupersonicAds/sonic-ftrl-api/blob/27d8815cf22bcc28c1b4de1bde5ef87bcae459a1/sequencing/representation_creator.ipynb. The result is a dictionary, having a vector of values (usually 70/300/512 in length) for each entity (device model). The results will be stored in pickle format where first vector (data[0]) is a list of embeddings and the 2nd vector (data[1]) is a list of corresponding device models.\n",
    "\n",
    "This notebook will load said data into the FeatureStore in a \"date aware fashion\", meaning, we will time stamp records during the ingestion process (All records ingested during the same run will have the same timestamp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix <A HREF=\"https://github.com/hdmf-dev/hdmf/issues/617\">panda/numpy incompatibility issues</A>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install wheel\n",
    "#!{sys.executable} -m pip install sagemaker pandas numpy numba s3fs --upgrade\n",
    "!{sys.executable} -m pip install sagemaker pandas numpy numba --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up boto client and the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "boto_client_s3 = boto3.client('s3', region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "boto_client_sagemaker = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "boto_client_featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=boto_client_sagemaker,\n",
    "    sagemaker_featurestore_runtime_client=boto_client_featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load representation data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "my_bucket = 'sagemaker-studio-ilya-test-20211221'\n",
    "my_file = 'input_data/xvocab.pkl'\n",
    "\n",
    "#boto_client_s3 = boto3.client('s3')\n",
    "response = boto_client_s3.get_object(Bucket=my_bucket, Key=my_file)\n",
    "body = response['Body']\n",
    "data = pickle.loads(body.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some debugging info\n",
    "\n",
    "d = {\n",
    "    'DeviceID': data[1],\n",
    "    'embeddings': data[0].tolist()\n",
    "}\n",
    "my_sample_data = pd.DataFrame(data=d)\n",
    "print (\"head():\\n\", my_sample_data.head())\n",
    "print (\"dtypes:\\n\",my_sample_data.dtypes)\n",
    "print (\"columns():\\n\")\n",
    "my_sample_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "my_bucket = 'sagemaker-studio-ilya-test-20211221'\n",
    "my_file = 'input_data/xvocab.schema.json'\n",
    "\n",
    "#boto_client_s3 = boto3.client('s3')\n",
    "response = boto_client_s3.get_object(Bucket=my_bucket, Key=my_file)\n",
    "body = response['Body']\n",
    "schema = json.loads(body.read())\n",
    "print (\"schema:\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe initialization record.\n",
    "First, we will create a dictionary which will be used to initialize Panda DataFrame object.\n",
    "Then we will create a DataFrame object and cast its each 'object' dtype column to string do it's ready for SageMaker FeatureStore SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_initialization_record(schema,data,current_time_sec=None):\n",
    "    answer = {}\n",
    "    if current_time_sec == None:\n",
    "        current_time_sec = int(round(time.time()))\n",
    "    for col in schema['features']:\n",
    "        if col[\"transformation\"] == \"tolist\":\n",
    "            index = col[\"index\"] # this column's index within `data` array\n",
    "            name = col['name']\n",
    "            value = data[index].tolist()\n",
    "            answer[name] = value\n",
    "        elif col[\"transformation\"] == \"time_now\":\n",
    "            print (\"skipping timenow for now\")\n",
    "            name = col['name']\n",
    "            value = pd.Series([current_time_sec]*len(data[0]), dtype=\"float64\")\n",
    "            answer[name] = value\n",
    "        else:\n",
    "            index = col[\"index\"]\n",
    "            name = col['name']\n",
    "            value = data[index]\n",
    "            answer[name] = value\n",
    "    return answer\n",
    "\n",
    "def cast_object_to_string(data_frame):\n",
    "    for label in data_frame.columns:\n",
    "        if data_frame.dtypes[label] == 'object':\n",
    "            data_frame[label] = data_frame[label].astype(\"str\").astype(\"string\")\n",
    "\n",
    "# Create a dictionary similar to this but using dynamically defined (via schema) columns\n",
    "# d2 = {\n",
    "#     'DeviceID': data[1],\n",
    "#     'embeddings': data[0].tolist()\n",
    "# }\n",
    "current_time_sec = int(round(time.time()))\n",
    "d2 = create_df_initialization_record(schema,data,current_time_sec)\n",
    "\n",
    "# Create a Panda DataFrame and cast it to satisfy SageMaker SDK requirements\n",
    "my_sample_data2 = pd.DataFrame(data=d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"head():\\n\", my_sample_data2.head())\n",
    "print (\"dtypes:\\n\",my_sample_data2.dtypes)\n",
    "print (\"columns():\\n\")\n",
    "my_sample_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_object_to_string(my_sample_data2)\n",
    "my_sample_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup SageMaker FeatureStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Bucket Setup For The OfflineStore\n",
    "\n",
    "SageMaker FeatureStore writes the data in the OfflineStore of a FeatureGroup to a S3 bucket owned by you. To be able to write to your S3 bucket, SageMaker FeatureStore assumes an IAM role which has access to it. The role is also owned by you.\n",
    "Note that the same bucket can be re-used across FeatureGroups. Data in the bucket is partitioned by FeatureGroup.\n",
    "\n",
    "Set the default s3 bucket name and it will be referenced throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the following to use a bucket of your choosing\n",
    "default_s3_bucket_name = feature_store_session.default_bucket() # default S3 bucket defined during SageMaker domain creation.\n",
    "default_s3_bucket_name = \"sagemaker-studio-ilya-test-20211221\" # we do not use a default S3 bucket defined during SageMaker domain creation.\n",
    "prefix = 'sagemaker-basic-featurestore-vecors-demo'\n",
    "\n",
    "print(default_s3_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the IAM role. \n",
    "This role gives SageMaker FeatureStore access to your S3 bucket. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> In this example we use the default SageMaker role, assuming it has both <b>AmazonSageMakerFullAccess</b> and <b>AmazonSageMakerFeatureStoreAccess</b> managed policies. If not, please make sure to attach them to the role before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "# You can modify the following to use a role of your choosing. See the documentation for how to create this.\n",
    "  # sagemaker_session = sagemaker.Session()\n",
    "  # role = sagemaker.get_execution_role()\n",
    "role = get_execution_role()\n",
    "if role != 'arn:aws:iam::032106861074:role/service-role/AmazonSageMaker-ExecutionRole-20181031T162966':\n",
    "    print(f\"Warning: you are running using '{role}' role.Trying to switch to AmazonSageMaker-ExecutionRole-20181031T162966\")\n",
    "    role = 'arn:aws:iam::032106861074:role/service-role/AmazonSageMaker-ExecutionRole-20181031T162966'\n",
    "role\n",
    "print (role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the FeatureGroup and create it if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define FeatureGroups\n",
    "The FeatureGroup name will include the timestamp; all other information should be pulled from the schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "my_sample_feature_group_name = 'deviceid-feature-group-' + strftime('%d-%H-%M-%S', gmtime()) # not sure yet how to deal with it best\n",
    "my_sample_feature_group_name = 'deviceid-feature-group' # we are going to store features from different runs in a single group and timestamp the features instead\n",
    "my_sample_feature_group = FeatureGroup(name=my_sample_feature_group_name, sagemaker_session=feature_store_session)\n",
    "\n",
    "# load feature definitions to the feature group. SageMaker FeatureStore Python SDK will auto-detect the data schema based on input data.\n",
    "my_sample_feature_group.load_feature_definitions(data_frame=my_sample_data2); # output is suppressed\n",
    "\n",
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = schema['column_record_id'] # \"DeviceID\"\n",
    "event_time_feature_name = schema['column_event_time'] # \"EventTime\"\n",
    "Tags = schema['tags']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create FeatureGroups in SageMaker FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "\n",
    "def featuregroup_already_exists(feature_group, boto_client_sagemaker):\n",
    "    response = boto_client_sagemaker.list_feature_groups()\n",
    "    #print (response)\n",
    "    list = [ item['FeatureGroupName'] for item in response[\"FeatureGroupSummaries\"]]\n",
    "    print(f\"Looking for feature group '{my_sample_feature_group.name}' in list {list}\")\n",
    "    if my_sample_feature_group.name in list:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "if featuregroup_already_exists(my_sample_feature_group,boto_client_sagemaker):\n",
    "    print (\"Skipping creation of Feature Group '{my_sample_feature_group.name}' as it already exists.\")\n",
    "else:\n",
    "    my_sample_feature_group.create(\n",
    "        s3_uri=f\"s3://{default_s3_bucket_name}/{prefix}\",\n",
    "        record_identifier_name=record_identifier_feature_name,\n",
    "        event_time_feature_name=event_time_feature_name,\n",
    "        tags=Tags,\n",
    "        role_arn=role,\n",
    "        enable_online_store=True\n",
    "    )\n",
    "    wait_for_feature_group_creation_complete(feature_group=my_sample_feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the FeatureGroup has been created by using the DescribeFeatureGroup and ListFeatureGroups APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_describe_response = my_sample_feature_group.describe()\n",
    "feature_group_describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion and manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PutRecords into FeatureGroup\n",
    "\n",
    "After the FeatureGroups have been created, we can put data into the FeatureGroups by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to S3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_feature_group.ingest(\n",
    "    data_frame=my_sample_data2, max_workers=8, wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that data has been ingested, we can quickly retrieve a record from the online store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_identifier_value = str(1)\n",
    "record_identifier_value = \"iphone13_4\"\n",
    "\n",
    "record = boto_client_featurestore_runtime.get_record(FeatureGroupName=my_sample_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)\n",
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pandas DataFrame from FeatureStore response - single record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record_identifier_value = str(3)\n",
    "record_identifier_value = \"iphone12_1\"\n",
    "\n",
    "record = boto_client_featurestore_runtime.get_record(FeatureGroupName=my_sample_feature_group_name, RecordIdentifierValueAsString=record_identifier_value)[\"Record\"]\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature_name_value(record):\n",
    "    result_dict = {}\n",
    "    for feature in record:\n",
    "        result_dict[feature[\"FeatureName\"]] = [feature[\"ValueAsString\"]]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_as_dict = map_feature_name_value(record)\n",
    "record_as_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=record_as_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch fetch multiple Product records from the Online Feature Store\n",
    "\n",
    "Fetch a list of selected items from the feature group.\n",
    "##### Up to 100 records can be fetched from an online Feature Store in a single batch operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiers = [\n",
    "    {\n",
    "        'FeatureGroupName': my_sample_feature_group_name,\n",
    "        'RecordIdentifiersValueAsString': [\"iphone11_8\", \"iphone13_4\",\"m1031g2\", \"hisense f15\"]\n",
    "    }\n",
    "]\n",
    "        \n",
    "batch_get_record_response = boto_client_featurestore_runtime.batch_get_record(Identifiers=identifiers)\n",
    "records = batch_get_record_response['Records']\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pandas DataFrame from FeatureStore response - multiple records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_definitions = my_sample_feature_group.describe()[\"FeatureDefinitions\"]\n",
    "feature_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature_name_value(records):\n",
    "    result_dict = {}\n",
    "    for feature in feature_definitions:\n",
    "        result_dict[feature[\"FeatureName\"]] = []\n",
    "\n",
    "    for record in records:\n",
    "        for feature in record[\"Record\"]:\n",
    "            result_dict[feature[\"FeatureName\"]].append(feature[\"ValueAsString\"])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_as_dict = map_feature_name_value(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = pd.DataFrame(data=records_as_dict)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outcomes and run a join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = 'sagemaker-studio-ilya-test-20211221'\n",
    "my_file = 'input_data/csv_outcomes_is_ilya2.csv'\n",
    "\n",
    "outcomes = pd.read_csv(f\"s3://{my_bucket}/{my_file}\")\n",
    "#outcomes = pd.read_csv('s3://sagemaker-studio-ilya-test-20211221/input_data/csv_outcomes_is_ilya2.csv')\n",
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = outcomes[[\"supply_app_bundle_id\",\"device_id\", \"device_model\"]]\n",
    "selected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_data2.head()\n",
    "a = my_sample_data2.rename(columns={'DeviceID': 'device_model'})\n",
    "a.loc[a['device_model'].isin(['sm-g973f','dammar','kyv48','b50pro','ptb10r'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= pd.merge(selected_columns,a,on=\"device_model\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample_feature_group.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "12a2d5ddd157b106bb479ba14d1b11656fa1dbf4e9c62fc64d499c1c72d4bda9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
